{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetBERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GetBERT, self).__init__()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"chinese-bert-wwm-ext\")\n",
    "        self.bert = BertModel.from_pretrained(\"chinese-bert-wwm-ext\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, sentence_lists):\n",
    "        \"\"\"\n",
    "        输入句子列表(去掉了停用词的)\n",
    "        \"\"\"\n",
    "        sentence_lists = [' '.join(x) for x in sentence_lists]\n",
    "        # print('sentence_lists:'+str(sentence_lists))\n",
    "        ids = self.bert_tokenizer(sentence_lists, padding=True, return_tensors=\"pt\")\n",
    "        # print('ids:'+str(ids))\n",
    "        inputs = ids['input_ids']\n",
    "        # print('inputs:'+str(inputs))\n",
    "\n",
    "        embeddings = self.bert(inputs)\n",
    "        # print(str(embeddings[0].shape))\n",
    "        x = embeddings[0]  # 1 * 768\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Pre:\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        输入一个文本\n",
    "        \"\"\"\n",
    "        self.puncs_coarse = ['。', '!', '；', '？', '……', '\\n', ' ']\n",
    "        self.text = text\n",
    "        self.stopwords = self.deal_wrap('dict/stop1205.txt')\n",
    "\n",
    "    def segment(self, sentence):\n",
    "        sentence_seged = jieba.cut(sentence.strip())\n",
    "        outstr = ''\n",
    "        for word in sentence_seged:\n",
    "            if word not in self.stopwords:\n",
    "                if word != '\\t':\n",
    "                    outstr += word\n",
    "                    outstr += \" \"\n",
    "        word_list = outstr.split(' ')\n",
    "        pattern = '[A-Za-z]*[0-9]*[\\'\\\"\\%.\\s\\@\\!\\#\\$\\^\\&\\*\\(\\)\\-\\<\\>\\?\\/\\,\\~\\`\\:\\;]*[：；”“ ‘’+-——！，。？、~@#￥%……&*（）【】]*'\n",
    "        t = [re.sub(pattern, \"\", x.strip()) for x in word_list]\n",
    "        t = [x for x in t if x != '']\n",
    "        return ''.join(t)\n",
    "\n",
    "    def deal_wrap(self, filedict):\n",
    "        temp = []\n",
    "        for x in open(filedict, 'r', encoding='utf-8').readlines():\n",
    "            temp.append(x.strip())\n",
    "        return temp\n",
    "\n",
    "    def split_sentence_coarse(self):\n",
    "        \"\"\"\n",
    "        按照。！？“”等中文完整句子语义来分句\n",
    "        1. 去除换行符、多余的空格、百分号\n",
    "        2. 分句，存入列表\n",
    "        :return:装着每个句子的列表（包括标点符号）\n",
    "        \"\"\"\n",
    "\n",
    "        text = self.text\n",
    "        sentences = []\n",
    "        start = 0\n",
    "        for i in range(len(text)):\n",
    "            if text[i] in self.puncs_coarse:\n",
    "                sentences.append(text[start:i + 1])\n",
    "                start = i + 1\n",
    "        if start == 0:\n",
    "            sentences.append(text)\n",
    "        return sentences\n",
    "\n",
    "    def get_keywords(self, data):\n",
    "        \"\"\"\n",
    "        如果句子太长，就进行关键词提取\n",
    "        \"\"\"\n",
    "        from jieba import analyse\n",
    "        textrank = analyse.textrank\n",
    "        keywords = textrank(data, topK=8)\n",
    "        return ''.join(keywords)\n",
    "\n",
    "    def preprocess(self):\n",
    "        # 分句\n",
    "        sentences = self.split_sentence_coarse()\n",
    "        # 对每个句子，去除里面的停用词，再连起来\n",
    "        # 对每个句子，如果句子太长，长度大于20（我随便定的），就抽取八个关键词连起来\n",
    "        new_sent = []\n",
    "        for i in sentences:\n",
    "            if len(i) < 5:\n",
    "                new_sent.append(i)\n",
    "                continue\n",
    "            i = self.segment(i)\n",
    "            if len(i) > 25:\n",
    "                i = self.get_keywords(i)\n",
    "            if len(i) > 20:\n",
    "                i = i[:20]\n",
    "            if i != '':\n",
    "                new_sent.append(i)\n",
    "        return new_sent\n",
    "\n",
    "\n",
    "class GetData():\n",
    "    def __init__(self, pos=4000, neg=3600):\n",
    "        data = pd.read_csv('sentiment_classify_data/raw_comment_v2.csv')\n",
    "        data = data[data['score'] != 3].reset_index()\n",
    "        data['label'] = data['score'].map(lambda a: 1 if a in [4, 5] else 0)\n",
    "        data.drop(['post_time','score','shop_url'],inplace=True,axis=1)\n",
    "        \n",
    "#         data = pd.read_excel('sentiment_classify_data/comments_raw_v1.xls')\n",
    "#         data = data[data['score'] != 3].reset_index()\n",
    "#         data['label'] = data['score'].map(lambda a: 1 if a in [4, 5] else 0)\n",
    "#         data.drop(['id', 'score'], inplace=True, axis=1)\n",
    "        \n",
    "        data['content'] = [str(i) for i in list(data['content'])]\n",
    "        # 原数据标签为0（负向情感）的数据有3632条，正向情感的有57262条\n",
    "        data1 = data[data['label'] == 1].sample(pos)\n",
    "        data0 = data[data['label'] == 0].sample(neg)\n",
    "        data = pd.concat([data1, data0], axis=0, ignore_index=True)\n",
    "        self.data = data\n",
    "\n",
    "    def split_sen(self):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in trange(len(self.data)):\n",
    "            p = Pre(self.data['content'][i])\n",
    "            sen_lst = p.preprocess()\n",
    "            if sen_lst == []:\n",
    "                continue\n",
    "            x.append(sen_lst)\n",
    "            y.append(self.data['label'][i])\n",
    "        print(len(x))\n",
    "        print(y.count(1))\n",
    "        print(y.count(0))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as copy\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "def seed_torch(seed=1122):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "BERT_PATH = 'bert-base-chinese/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, train_dataLoader, test_dataLoader):\n",
    "    # 训练模型\n",
    "    best_model = None\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    best_loss = 100\n",
    "    epoch_cnt = 0\n",
    "    for _ in range(epoch):\n",
    "        total_train_loss = 0\n",
    "        total_train_num = 0\n",
    "        total_test_loss = 0\n",
    "        total_test_num = 0\n",
    "        for x, y in tqdm(train_dataLoader,\n",
    "                         desc='Epoch: {}| Train Loss: {}| Test Loss: {}'.format(_, train_loss, test_loss)):\n",
    "        #for x, y in train_dataLoader:\n",
    "            x_num = len(x)\n",
    "            p = model(x,model.init_hidden())\n",
    "            loss = loss_func(p, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_num += x_num\n",
    "        train_loss = total_train_loss / total_train_num\n",
    "        train_loss_list.append(train_loss)\n",
    "        for x, y in test_dataLoader:\n",
    "            x_num = len(x)\n",
    "            p = model(x,model.init_hidden())\n",
    "            loss = loss_func(p, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_num += x_num\n",
    "        test_loss = total_test_loss / total_test_num\n",
    "        test_loss_list.append(test_loss)\n",
    "        \n",
    "        # early stop\n",
    "        if best_loss > test_loss:\n",
    "            best_loss = test_loss\n",
    "            best_model = copy(model)\n",
    "            torch.save(best_model.state_dict(), 'bigru.pth')\n",
    "            epoch_cnt = 0\n",
    "        else:\n",
    "            epoch_cnt += 1\n",
    "            \n",
    "        if epoch_cnt > early_stop:\n",
    "            torch.save(best_model.state_dict(), 'bigru.pth')\n",
    "            print(\"保存模型\")\n",
    "            #print(best_model.state_dict())\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_dataLoader_):\n",
    "    pred = []\n",
    "    label = []\n",
    "    model_.load_state_dict(torch.load(\"bigru.pth\"))\n",
    "    model_.eval()\n",
    "    total_test_loss = 0\n",
    "    total_test_num = 0\n",
    "    for x, y in test_dataLoader_:\n",
    "        x_num = len(x)\n",
    "        p = model_(x,model_.init_hidden())\n",
    "        #         print('##', len(p), len(y))\n",
    "        loss = loss_func(p, y.long())\n",
    "        total_test_loss += loss.item()\n",
    "        total_test_num += x_num\n",
    "        pred.extend(p.data.squeeze(1).tolist())\n",
    "        label.extend(y.tolist())\n",
    "    test_loss = total_test_loss / total_test_num\n",
    "    # print('##', len(pred), len(label))\n",
    "    return pred, label, test_loss, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(torch.nn.Module):\n",
    "    def __init__(self, input_dim=768,hidden_size=768, out_size=2, n_layers=1, batch_size=16):\n",
    "        super(BiGRU, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.out_size = out_size\n",
    "        self.gru = torch.nn.GRU(input_dim, hidden_size, n_layers, batch_first=True,bidirectional=True)\n",
    "        self.fc1 = torch.nn.Linear(hidden_size*2, 300)\n",
    "        self.fc2 = torch.nn.Linear(300, out_size)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        self.batch_size = word_inputs.shape[0]\n",
    "       # hidden 就是上下文输出，output 就是 RNN 输出\n",
    "        output, hidden = self.gru(word_inputs, hidden)\n",
    "        # output是所有隐藏层的状态，hidden是最后一层隐藏层的状态\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        # 仅仅获取 time seq 维度中的最后一个向量\n",
    "        # the last of time_seq\n",
    "        output = output[:,-1,:]\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.autograd.Variable(torch.zeros(2*self.n_layers, self.batch_size, self.hidden_size))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.802 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 6000/6000 [00:17<00:00, 348.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5987\n",
      "2995\n",
      "2992\n",
      "训练集有5387个数据\n",
      "测试集有600个数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Train Loss: 0| Test Loss: 0:   0%|          | 0/336 [00:00<?, ?it/s]/app/common/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0: 100%|██████████| 336/336 [05:59<00:00,  1.07s/it]\n",
      "Epoch: 1| Train Loss: 0.019483640937583653| Test Loss: 0.06446073078424544: 100%|██████████| 336/336 [05:51<00:00,  1.05s/it]\n",
      "Epoch: 2| Train Loss: 0.02336034977079613| Test Loss: 0.06115482833135773: 100%|██████████| 336/336 [05:42<00:00,  1.02s/it]\n",
      "Epoch: 3| Train Loss: 0.023567904653083088| Test Loss: 0.05827723962028284: 100%|██████████| 336/336 [05:49<00:00,  1.04s/it]\n",
      "Epoch: 4| Train Loss: 0.02362254196209722| Test Loss: 0.055835924589553394: 100%|██████████| 336/336 [05:55<00:00,  1.06s/it]\n",
      "Epoch: 5| Train Loss: 0.023585950208057295| Test Loss: 0.05374622158706188: 100%|██████████| 336/336 [06:02<00:00,  1.08s/it]\n",
      "Epoch: 6| Train Loss: 0.023499752357435812| Test Loss: 0.05195219668786268: 100%|██████████| 336/336 [05:49<00:00,  1.04s/it]\n",
      "Epoch: 7| Train Loss: 0.023395000174613892| Test Loss: 0.05041518681556792: 100%|██████████| 336/336 [05:52<00:00,  1.05s/it]\n",
      "Epoch: 8| Train Loss: 0.023292630768126054| Test Loss: 0.049105658070058435: 100%|██████████| 336/336 [05:55<00:00,  1.06s/it]\n",
      "Epoch: 9| Train Loss: 0.02320493263098234| Test Loss: 0.04799813510396996: 100%|██████████| 336/336 [06:11<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49324324324324326\n"
     ]
    }
   ],
   "source": [
    "seed_torch(22)\n",
    "epoch = 10\n",
    "batch_size = 16\n",
    "early_stop = 5\n",
    "test_loss_list = []\n",
    "train_loss_list = []\n",
    "\n",
    "# 初始化模型\n",
    "model = BiGRU()\n",
    "model_ = BiGRU()\n",
    "\n",
    "# 数据处理部分\n",
    "gd = GetData(pos=3000, neg=3000)\n",
    "x, y = gd.split_sen()\n",
    "pos = y.count(1)\n",
    "neg = y.count(0)\n",
    "pos_train = int(pos*0.9)\n",
    "neg_train = int(neg*0.9)\n",
    "x1 = x[:pos]  # 3988   --- 3589train 399test\n",
    "y1 = y[:pos]\n",
    "x0 = x[pos:]  # 3589   ---- 3230train 359test\n",
    "y0 = y[pos:]\n",
    "\n",
    "train_x = x0[:neg_train] + x1[:pos_train]\n",
    "train_y = y0[:neg_train] + y1[:pos_train]\n",
    "print(\"训练集有\"+str(len(train_x))+\"个数据\")\n",
    "\n",
    "# c = list(zip(train_x, train_y))\n",
    "# random.shuffle(c)\n",
    "# c = random.sample(c, 50)\n",
    "# train_x[:], train_y[:] = zip(*c)\n",
    "\n",
    "test_x = x0[neg_train:] + x1[pos_train:]\n",
    "test_y = y0[neg_train:] + y1[pos_train:]\n",
    "print(\"测试集有\"+str(len(test_x))+\"个数据\")\n",
    "\n",
    "bert = GetBERT()\n",
    "x_train = bert(train_x)\n",
    "x_test = bert(test_x)\n",
    "y_train = torch.tensor(train_y).float()\n",
    "y_test = torch.tensor(test_y).float()\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_dataLoader = DataLoader(train_data, batch_size=batch_size,drop_last=True)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "test_dataLoader = DataLoader(test_data, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "# 损失函数和优化器\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "train_model(epoch, train_dataLoader, test_dataLoader)\n",
    "p, y, test_loss, test_loss_list = test_model(test_dataLoader)\n",
    "ans = []\n",
    "for t in p:\n",
    "    if t[0]>t[1]:\n",
    "        ans.append(0)\n",
    "    else:\n",
    "        ans.append(1)\n",
    "print(accuracy_score(ans,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
