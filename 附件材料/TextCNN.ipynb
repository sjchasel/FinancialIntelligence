{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as copy\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "def seed_torch(seed=1122):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "BERT_PATH = 'bert-base-chinese/'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetBERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GetBERT, self).__init__()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"chinese-bert-wwm-ext\")\n",
    "        self.bert = BertModel.from_pretrained(\"chinese-bert-wwm-ext\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, sentence_lists):\n",
    "        \"\"\"\n",
    "        输入句子列表(去掉了停用词的)\n",
    "        \"\"\"\n",
    "        sentence_lists = [' '.join(x) for x in sentence_lists]\n",
    "        # print('sentence_lists:'+str(sentence_lists))\n",
    "        ids = self.bert_tokenizer(sentence_lists, padding=True, return_tensors=\"pt\")\n",
    "        # print('ids:'+str(ids))\n",
    "        inputs = ids['input_ids']\n",
    "        # print('inputs:'+str(inputs))\n",
    "\n",
    "        embeddings = self.bert(inputs)\n",
    "        # print(str(embeddings[0].shape))\n",
    "        x = embeddings[0]  # 1 * 768\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Pre:\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        输入一个文本\n",
    "        \"\"\"\n",
    "        self.puncs_coarse = ['。', '!', '；', '？', '……', '\\n', ' ']\n",
    "        self.text = text\n",
    "        self.stopwords = self.deal_wrap('dict/stop1205.txt')\n",
    "\n",
    "    def segment(self, sentence):\n",
    "        sentence_seged = jieba.cut(sentence.strip())\n",
    "        outstr = ''\n",
    "        for word in sentence_seged:\n",
    "            if word not in self.stopwords:\n",
    "                if word != '\\t':\n",
    "                    outstr += word\n",
    "                    outstr += \" \"\n",
    "        word_list = outstr.split(' ')\n",
    "        pattern = '[A-Za-z]*[0-9]*[\\'\\\"\\%.\\s\\@\\!\\#\\$\\^\\&\\*\\(\\)\\-\\<\\>\\?\\/\\,\\~\\`\\:\\;]*[：；”“ ‘’+-——！，。？、~@#￥%……&*（）【】]*'\n",
    "        t = [re.sub(pattern, \"\", x.strip()) for x in word_list]\n",
    "        t = [x for x in t if x != '']\n",
    "        return ''.join(t)\n",
    "\n",
    "    def deal_wrap(self, filedict):\n",
    "        temp = []\n",
    "        for x in open(filedict, 'r', encoding='utf-8').readlines():\n",
    "            temp.append(x.strip())\n",
    "        return temp\n",
    "\n",
    "    def split_sentence_coarse(self):\n",
    "        \"\"\"\n",
    "        按照。！？“”等中文完整句子语义来分句\n",
    "        1. 去除换行符、多余的空格、百分号\n",
    "        2. 分句，存入列表\n",
    "        :return:装着每个句子的列表（包括标点符号）\n",
    "        \"\"\"\n",
    "\n",
    "        text = self.text\n",
    "        sentences = []\n",
    "        start = 0\n",
    "        for i in range(len(text)):\n",
    "            if text[i] in self.puncs_coarse:\n",
    "                sentences.append(text[start:i + 1])\n",
    "                start = i + 1\n",
    "        if start == 0:\n",
    "            sentences.append(text)\n",
    "        return sentences\n",
    "\n",
    "    def get_keywords(self, data):\n",
    "        \"\"\"\n",
    "        如果句子太长，就进行关键词提取\n",
    "        \"\"\"\n",
    "        from jieba import analyse\n",
    "        textrank = analyse.textrank\n",
    "        keywords = textrank(data, topK=8)\n",
    "        return ''.join(keywords)\n",
    "\n",
    "    def preprocess(self):\n",
    "        # 分句\n",
    "        sentences = self.split_sentence_coarse()\n",
    "        # 对每个句子，去除里面的停用词，再连起来\n",
    "        # 对每个句子，如果句子太长，长度大于20（我随便定的），就抽取八个关键词连起来\n",
    "        new_sent = []\n",
    "        for i in sentences:\n",
    "            if len(i) < 5:\n",
    "                new_sent.append(i)\n",
    "                continue\n",
    "            i = self.segment(i)\n",
    "            if len(i) > 25:\n",
    "                i = self.get_keywords(i)\n",
    "            if i != '':\n",
    "                new_sent.append(i)\n",
    "        return new_sent\n",
    "\n",
    "\n",
    "class GetData():\n",
    "    def __init__(self, pos=4000, neg=3600):\n",
    "        data = pd.read_csv('sentiment_classify_data/raw_comment_v2.csv')\n",
    "        data = data[data['score'] != 3].reset_index()\n",
    "        data['label'] = data['score'].map(lambda a: 1 if a in [4, 5] else 0)\n",
    "        data.drop(['post_time','score','shop_url'],inplace=True,axis=1)\n",
    "        \n",
    "#         data = pd.read_excel('sentiment_classify_data/comments_raw_v1.xls')\n",
    "#         data = data[data['score'] != 3].reset_index()\n",
    "#         data['label'] = data['score'].map(lambda a: 1 if a in [4, 5] else 0)\n",
    "#         data.drop(['id', 'score'], inplace=True, axis=1)\n",
    "        \n",
    "        data['content'] = [str(i) for i in list(data['content'])]\n",
    "        # 原数据标签为0（负向情感）的数据有3632条，正向情感的有57262条\n",
    "        data1 = data[data['label'] == 1].sample(pos)\n",
    "        data0 = data[data['label'] == 0].sample(neg)\n",
    "        data = pd.concat([data1, data0], axis=0, ignore_index=True)\n",
    "        self.data = data\n",
    "\n",
    "    def split_sen(self):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in trange(len(self.data)):\n",
    "            p = Pre(self.data['content'][i])\n",
    "            sen_lst = p.preprocess()\n",
    "            if sen_lst == []:\n",
    "                continue\n",
    "            x.append(sen_lst)\n",
    "            y.append(self.data['label'][i])\n",
    "        print(len(x))\n",
    "        print(y.count(1))\n",
    "        print(y.count(0))\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm_layer = nn.LSTM(input_size=768, hidden_size=128, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(in_features=128, out_features=2, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1, (h_n, h_c) = self.lstm_layer(x)\n",
    "        a, b, c = h_n.shape\n",
    "        out = self.linear_layer(h_n.reshape(a * b, c))\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(epoch, train_dataLoader, test_dataLoader):\n",
    "    # 训练模型\n",
    "    best_model = None\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    best_loss = 100\n",
    "    epoch_cnt = 0\n",
    "    for _ in range(epoch):\n",
    "        total_train_loss = 0\n",
    "        total_train_num = 0\n",
    "        total_test_loss = 0\n",
    "        total_test_num = 0\n",
    "        for x, y in tqdm(train_dataLoader,\n",
    "                         desc='Epoch: {}| Train Loss: {}| Test Loss: {}'.format(_, train_loss, test_loss)):\n",
    "            # for x, y in train_dataLoader:\n",
    "            x_num = len(x)\n",
    "            p = model(x)\n",
    "            loss = loss_func(p, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            total_train_loss += float(loss.item())\n",
    "            total_train_num += x_num\n",
    "        train_loss = total_train_loss / total_train_num\n",
    "        train_loss_list.append(train_loss)\n",
    "        for x, y in test_dataLoader:\n",
    "            x_num = len(x)\n",
    "            p = model(x)\n",
    "            loss = loss_func(p, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            total_test_loss += float(loss.item())\n",
    "            total_test_num += x_num\n",
    "        test_loss = total_test_loss / total_test_num\n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        # early stop\n",
    "        if best_loss > test_loss:\n",
    "            best_loss = test_loss\n",
    "            best_model = copy(model)\n",
    "            torch.save(best_model.state_dict(), 'textcnn.pth')\n",
    "            epoch_cnt = 0\n",
    "        else:\n",
    "            epoch_cnt += 1\n",
    "\n",
    "        if epoch_cnt > early_stop:\n",
    "            torch.save(best_model.state_dict(), 'textcnn.pth')\n",
    "            print(\"保存模型\")\n",
    "            # print(best_model.state_dict())\n",
    "            break\n",
    "def test_model(test_dataLoader_):\n",
    "    pred = []\n",
    "    label = []\n",
    "    model_.load_state_dict(torch.load(\"textcnn.pth\"))\n",
    "    model_.eval()\n",
    "    total_test_loss = 0\n",
    "    total_test_num = 0\n",
    "    for x, y in test_dataLoader_:\n",
    "        x_num = len(x)\n",
    "        p = model_(x)\n",
    "#         print('##', len(p), len(y))\n",
    "        loss = loss_func(p, y.long())\n",
    "        total_test_loss += loss.item()\n",
    "        total_test_num += x_num\n",
    "        pred.extend(p.data.squeeze(1).tolist())\n",
    "        label.extend(y.tolist())\n",
    "    test_loss = total_test_loss / total_test_num\n",
    "    # print('##', len(pred), len(label))\n",
    "    return pred, label, test_loss, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def matrix_mul(inputs, weight, bias=None):\n",
    "#     print('inputs:'+str(inputs.shape))#torch.Size([20, 12, 32])\n",
    "#     print('weight'+str(weight.shape))# 32 * 1500\n",
    "    feature_list = []\n",
    "    for input in inputs:\n",
    "#         print('input'+str(input.shape)) # torch.Size([12, 32])\n",
    "        feature = torch.mm(input, weight)  # (T, C)*(C, A) = (T, A)\n",
    "        if isinstance(bias, torch.nn.parameter.Parameter):\n",
    "            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n",
    "        feature = torch.tanh(feature)\n",
    "        feature_list.append(feature)\n",
    "    return torch.stack(feature_list, 0).squeeze()  # (B, T)\n",
    "\n",
    "\n",
    "def wise_mul(inputs, alphas):\n",
    "    feature_list = []\n",
    "    for sequence, alpha in zip(inputs, alphas):\n",
    "        alpha = alpha.unsqueeze(1)\n",
    "        feature = sequence * alpha\n",
    "        feature_list.append(feature)\n",
    "    output = torch.stack(feature_list, 0)\n",
    "    return torch.sum(output, 1)\n",
    "\n",
    "\n",
    "def attention(inputs, attention_size):\n",
    "    \"\"\"\n",
    "    :param inputs: (batch_size, time_steps, hidden_size)\n",
    "    \"\"\"\n",
    "#     print('attention inputs:'+str(inputs.shape)) #torch.Size([20, 12, 32])\n",
    "    hidden_size = inputs.shape[2]\n",
    "    w = nn.Parameter(torch.randn(hidden_size, attention_size)) # 32 1500\n",
    "    b = nn.Parameter(torch.randn(1, attention_size)) # 1 1500 bias\n",
    "    u = nn.Parameter(torch.randn(attention_size, 1)) # 1500 1\n",
    "\n",
    "    if use_cuda:\n",
    "        w = w.cuda()\n",
    "        b = b.cuda()\n",
    "        u = u.cuda()\n",
    "\n",
    "    v = matrix_mul(inputs, w, b)       # (B, T, A) 32 12 1500\n",
    "    u_v = matrix_mul(v,u)              # (B, T) 32 12 1\n",
    "    alphas = F.softmax(u_v)            # (B, T)\n",
    "    output = wise_mul(inputs, alphas)  # (B, H)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,word_embedding_dimension=768,filters=32,kernel_size=[1,2,3,4]):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_channels=word_embedding_dimension,\n",
    "                               out_channels=filters,\n",
    "                               kernel_size=kernel_size[0])\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=word_embedding_dimension,\n",
    "                               out_channels=filters,\n",
    "                               kernel_size=kernel_size[1])\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=word_embedding_dimension,\n",
    "                               out_channels=filters,\n",
    "                               kernel_size=kernel_size[2])\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=word_embedding_dimension,\n",
    "                               out_channels=filters,\n",
    "                               kernel_size=kernel_size[3])\n",
    "\n",
    "        self.linear = nn.Linear(in_features=32, out_features=2)\n",
    "\n",
    "        self.batch_0 = nn.BatchNorm1d(num_features=12)\n",
    "        self.batch_1 = nn.BatchNorm1d(num_features=32)\n",
    "\n",
    "    def k_max_pooling(self, x, dim=2, k=3):\n",
    "        index = x.topk(k, dim=dim)[1].sort(dim=dim)[0]\n",
    "        return x.gather(dim, index)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print('-----------forward----------------')\n",
    "#         print(x.shape)#torch.Size([20, 122, 768])\n",
    "        batch_size = x.size(0)\n",
    "        embeddings = x.permute(0, 2, 1)\n",
    "#         print(embeddings.shape)#torch.Size([20, 768, 122])\n",
    "        x0 = self.conv0(embeddings)\n",
    "        x0 = F.relu(x0)\n",
    "        x0 = self.k_max_pooling(x0)\n",
    "#         print(x0.shape)#torch.Size([20, 32, 3])\n",
    "        x1 = self.conv1(embeddings)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.k_max_pooling(x1)\n",
    "#         print(x1.shape)#torch.Size([20, 32, 3])\n",
    "        x2 = self.conv2(embeddings)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.k_max_pooling(x2)\n",
    "#         print(x2.shape)#torch.Size([20, 32, 3])\n",
    "        x3 = self.conv3(embeddings)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.k_max_pooling(x3)\n",
    "#         print(x3.shape)#torch.Size([20, 32, 3])\n",
    "        x = torch.cat((x0, x1, x2, x3), dim=2).permute(0, 2, 1)\n",
    "#         print(x.shape)#torch.Size([20, 12, 32])\n",
    "        x = self.batch_0(x)\n",
    "#         print(x.shape)#torch.Size([20, 12, 32])\n",
    "        x = attention(x, 1500)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.batch_1(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:15<00:00, 654.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9971\n",
      "4988\n",
      "4983\n",
      "训练集有8973个数据\n",
      "测试集有998个数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Train Loss: 0| Test Loss: 0:   0%|          | 0/8973 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "self must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e743dc255719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d77d2024c5be>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epoch, train_dataLoader, test_dataLoader)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# for x, y in train_dataLoader:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mx_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/common/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9d2be8c0a16b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#         print(x.shape)#torch.Size([20, 12, 32])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8463df05550d>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(inputs, attention_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# (B, T, A) 32 12 1500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mu_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# (B, T) 32 12 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_v\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# (B, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwise_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8463df05550d>\u001b[0m in \u001b[0;36mmatrix_mul\u001b[0;34m(inputs, weight, bias)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#         print('input'+str(input.shape)) # torch.Size([12, 32])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (T, C)*(C, A) = (T, A)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: self must be a matrix"
     ]
    }
   ],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "seed_torch(22)\n",
    "epoch = 5\n",
    "batch_size = 1\n",
    "early_stop = 3\n",
    "test_loss_list = []\n",
    "train_loss_list = []\n",
    "\n",
    "model = TextCNN()\n",
    "model_ = TextCNN()\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    model_ = model_.cuda()\n",
    "\n",
    "\n",
    "# 数据处理部分\n",
    "gd = GetData(pos=5000, neg=5000)\n",
    "x, y = gd.split_sen()\n",
    "pos = y.count(1)\n",
    "neg = y.count(0)\n",
    "pos_train = int(pos*0.9)\n",
    "neg_train = int(neg*0.9)\n",
    "x1 = x[:pos]  # 3988   --- 3589train 399test\n",
    "y1 = y[:pos]\n",
    "x0 = x[pos:]  # 3589   ---- 3230train 359test\n",
    "y0 = y[pos:]\n",
    "\n",
    "train_x = x0[:neg_train] + x1[:pos_train]\n",
    "train_y = y0[:neg_train] + y1[:pos_train]\n",
    "print(\"训练集有\"+str(len(train_x))+\"个数据\")\n",
    "\n",
    "# c = list(zip(train_x, train_y))\n",
    "# random.shuffle(c)\n",
    "# c = random.sample(c, 50)\n",
    "# train_x[:], train_y[:] = zip(*c)\n",
    "\n",
    "test_x = x0[neg_train:] + x1[pos_train:]\n",
    "test_y = y0[neg_train:] + y1[pos_train:]\n",
    "print(\"测试集有\"+str(len(test_x))+\"个数据\")\n",
    "\n",
    "bert = GetBERT()\n",
    "x_train = bert(train_x)\n",
    "x_test = bert(test_x)\n",
    "y_train = torch.tensor(train_y).float()\n",
    "y_test = torch.tensor(test_y).float()\n",
    "\n",
    "if use_cuda:\n",
    "    x_train = x_train.cuda()\n",
    "    x_test = x_test.cuda()\n",
    "    y_train = y_train.cuda()\n",
    "    y_test = y_test.cuda()\n",
    "\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_dataLoader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "test_dataLoader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# 损失函数和优化器\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "if use_cuda:\n",
    "    loss_func = loss_func.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "train_model(epoch, train_dataLoader, test_dataLoader)\n",
    "p, y, test_loss, test_loss_list = test_model(test_dataLoader)\n",
    "ans = []\n",
    "for t in p:\n",
    "    if t[0]<t[1]:\n",
    "        ans.append(0)\n",
    "    else:\n",
    "        ans.append(1)\n",
    "print(accuracy_score(ans,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.plot(test_loss_list)\n",
    "plt.plot(train_loss_list)\n",
    "plt.legend([\"test\",\"train\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
