{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T00:37:38.708616Z",
     "start_time": "2021-06-22T00:37:33.602754Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Version' object has no attribute 'major'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9afa08e0e2d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m from .file_utils import (\n\u001b[0;32m     45\u001b[0m     \u001b[0m_BaseLazyModule\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpkg\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"tokenizers\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# must be loaded here, or else tqdm check may fail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_torch_available\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[0mtorch_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportlib_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"torch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m     _torch_fx_available = (torch_version.major, torch_version.minor) == (\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[0mTORCH_FX_REQUIRED_VERSION\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mTORCH_FX_REQUIRED_VERSION\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Version' object has no attribute 'major'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy as copy\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "def seed_torch(seed=1122):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T00:37:38.724580Z",
     "start_time": "2021-06-22T00:37:38.712613Z"
    }
   },
   "outputs": [],
   "source": [
    "BERT_PATH = 'bert-base-chinese/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T00:37:41.142381Z",
     "start_time": "2021-06-22T00:37:41.120440Z"
    }
   },
   "outputs": [],
   "source": [
    "class GetBERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GetBERT, self).__init__()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"C:/Users/12968/Desktop/chinese-bert-wwm-ext\")\n",
    "        self.bert = BertModel.from_pretrained(\"C:/Users/12968/Desktop/chinese-bert-wwm-ext\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    def forward(self, sentence_lists):\n",
    "        \"\"\"\n",
    "        输入句子列表(去掉了停用词的)\n",
    "        \"\"\"\n",
    "        sentence_lists = [' '.join(x) for x in sentence_lists]\n",
    "        #print('sentence_lists:'+str(sentence_lists))\n",
    "        ids = self.bert_tokenizer(sentence_lists, padding=True, return_tensors=\"pt\")\n",
    "        #print('ids:'+str(ids))\n",
    "        inputs = ids['input_ids']\n",
    "        #print('inputs:'+str(inputs))\n",
    "\n",
    "        embeddings = self.bert(inputs)\n",
    "        #print(str(embeddings[0].shape))\n",
    "        x = embeddings[0] #1 * 768\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T00:37:43.562233Z",
     "start_time": "2021-06-22T00:37:43.532281Z"
    }
   },
   "outputs": [],
   "source": [
    "class Pre:\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        输入一个文本\n",
    "        \"\"\"\n",
    "        self.puncs_coarse = ['。', '!', '；', '？', '……', '\\n',' ']\n",
    "        self.text = text\n",
    "        self.stopwords = self.deal_wrap('dict/stop1205.txt')\n",
    "    \n",
    "    def segment(self, sentence):\n",
    "        sentence_seged = jieba.cut(sentence.strip())\n",
    "        outstr = ''\n",
    "        for word in sentence_seged:\n",
    "            if word not in self.stopwords:\n",
    "                if word != '\\t':\n",
    "                    outstr += word\n",
    "                    outstr += \" \"\n",
    "        word_list = outstr.split(' ')\n",
    "        pattern = '[A-Za-z]*[0-9]*[\\'\\\"\\%.\\s\\@\\!\\#\\$\\^\\&\\*\\(\\)\\-\\<\\>\\?\\/\\,\\~\\`\\:\\;]*[：；”“ ‘’+-——！，。？、~@#￥%……&*（）【】]*'\n",
    "        t = [re.sub(pattern, \"\", x.strip()) for x in word_list]\n",
    "        t = [x for x in t if x != '']\n",
    "        return ''.join(t)\n",
    "    \n",
    "    def deal_wrap(self, filedict):\n",
    "        temp = []\n",
    "        for x in open(filedict, 'r', encoding='utf-8').readlines():\n",
    "            temp.append(x.strip())\n",
    "        return temp\n",
    "        \n",
    "    def split_sentence_coarse(self):\n",
    "        \"\"\"\n",
    "        按照。！？“”等中文完整句子语义来分句\n",
    "        1. 去除换行符、多余的空格、百分号\n",
    "        2. 分句，存入列表\n",
    "        :return:装着每个句子的列表（包括标点符号）\n",
    "        \"\"\"\n",
    "        \n",
    "        text = self.text\n",
    "        sentences = []\n",
    "        start = 0\n",
    "        for i in range(len(text)):\n",
    "            if text[i] in self.puncs_coarse:\n",
    "                sentences.append(text[start:i + 1])\n",
    "                start = i + 1\n",
    "        if start == 0:\n",
    "            sentences.append(text)\n",
    "        return sentences\n",
    "    \n",
    "    def get_keywords(self, data):\n",
    "        \"\"\"\n",
    "        如果句子太长，就进行关键词提取\n",
    "        \"\"\"\n",
    "        from jieba import analyse\n",
    "        textrank = analyse.textrank\n",
    "        keywords = textrank(data, topK=8)\n",
    "        return ''.join(keywords)\n",
    "\n",
    "    def preprocess(self):\n",
    "        # 分句\n",
    "        sentences = self.split_sentence_coarse()\n",
    "        # 对每个句子，去除里面的停用词，再连起来\n",
    "        # 对每个句子，如果句子太长，长度大于20（我随便定的），就抽取八个关键词连起来\n",
    "        new_sent = []\n",
    "        for i in sentences:\n",
    "            if len(i) < 5:\n",
    "                new_sent.append(i)\n",
    "                continue\n",
    "            i = self.segment(i)\n",
    "            if len(i) > 25:\n",
    "                i = self.get_keywords(i)\n",
    "            if i != '':\n",
    "                new_sent.append(i)\n",
    "        return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T00:37:47.372754Z",
     "start_time": "2021-06-22T00:37:47.351810Z"
    }
   },
   "outputs": [],
   "source": [
    "class GetData():\n",
    "    def __init__(self,pos=4000, neg=3600):\n",
    "        data = pd.read_excel('E:/FinancialIntelligence/sentiment_classify_data/comments_raw_v1.xls')\n",
    "        data = data[data['score']!=3].reset_index()\n",
    "        data['label'] = data['score'].map(lambda a : 1 if a in [4,5] else 0) \n",
    "        data.drop(['id','score'],inplace=True,axis=1)\n",
    "        data['content'] = [str(i) for i in list(data['content'])]\n",
    "        # 原数据标签为0（负向情感）的数据有3632条，正向情感的有57262条\n",
    "        data1 = data[data['label']==1].sample(pos)\n",
    "        data0 = data[data['label']==0].sample(neg)\n",
    "        data = pd.concat([data1,data0],axis=0,ignore_index=True)\n",
    "        self.data = data\n",
    "    \n",
    "    def split_sen(self):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in trange(len(self.data)):\n",
    "            p = Pre(self.data['content'][i])\n",
    "            sen_lst = p.preprocess()\n",
    "            if sen_lst == []:\n",
    "                continue\n",
    "            x.append(sen_lst)\n",
    "            y.append(self.data['label'][i])\n",
    "        print(len(x))\n",
    "        print(y.count(1))\n",
    "        print(y.count(0))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T00:37:49.388569Z",
     "start_time": "2021-06-22T00:37:49.380592Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm_layer = nn.LSTM(input_size=768, hidden_size=128, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(in_features=128, out_features=2, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1, (h_n, h_c) = self.lstm_layer(x)\n",
    "        a, b, c = h_n.shape\n",
    "        out = self.linear_layer(h_n.reshape(a*b, c))\n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T00:37:51.274266Z",
     "start_time": "2021-06-22T00:37:51.257308Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(epoch, train_dataLoader, test_dataLoader):\n",
    "    # 训练模型\n",
    "    best_model = None\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    best_loss = 100\n",
    "    epoch_cnt = 0\n",
    "    for _ in range(epoch):\n",
    "        total_train_loss = 0\n",
    "        total_train_num = 0\n",
    "        total_test_loss = 0\n",
    "        total_test_num = 0\n",
    "        for x, y in tqdm(train_dataLoader,\n",
    "                         desc='Epoch: {}| Train Loss: {}| Test Loss: {}'.format(_, train_loss, test_loss)):\n",
    "        #for x, y in train_dataLoader:\n",
    "            x_num = len(x)\n",
    "            p = model(x)\n",
    "            loss = loss_func(p, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_num += x_num\n",
    "        train_loss = total_train_loss / total_train_num\n",
    "        train_loss_list.append(train_loss)\n",
    "        for x, y in test_dataLoader:\n",
    "            x_num = len(x)\n",
    "            p = model(x)\n",
    "            loss = loss_func(p, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_num += x_num\n",
    "        test_loss = total_test_loss / total_test_num\n",
    "        test_loss_list.append(test_loss)\n",
    "        \n",
    "        # early stop\n",
    "        if best_loss > test_loss:\n",
    "            best_loss = test_loss\n",
    "            best_model = copy(model)\n",
    "            torch.save(best_model.state_dict(), 'lstm_.pth')\n",
    "            epoch_cnt = 0\n",
    "        else:\n",
    "            epoch_cnt += 1\n",
    "            \n",
    "        if epoch_cnt > early_stop:\n",
    "            torch.save(best_model.state_dict(), 'lstm_.pth')\n",
    "            print(\"保存模型\")\n",
    "            #print(best_model.state_dict())\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T00:37:54.394078Z",
     "start_time": "2021-06-22T00:37:54.386096Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(test_dataLoader_):\n",
    "    pred = []\n",
    "    label = []\n",
    "    model_.load_state_dict(torch.load(\"lstm_.pth\"))\n",
    "    model_.eval()\n",
    "    total_test_loss = 0\n",
    "    total_test_num = 0\n",
    "    for x, y in test_dataLoader_:\n",
    "        x_num = len(x)\n",
    "        p = model_(x)\n",
    "#         print('##', len(p), len(y))\n",
    "        loss = loss_func(p, y.long())\n",
    "        total_test_loss += loss.item()\n",
    "        total_test_num += x_num\n",
    "        pred.extend(p.data.squeeze(1).tolist())\n",
    "        label.extend(y.tolist())\n",
    "    test_loss = total_test_loss / total_test_num\n",
    "    # print('##', len(pred), len(label))\n",
    "    return pred, label, test_loss, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T08:23:16.515972Z",
     "start_time": "2021-06-22T07:29:10.804201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                      | 0/7600 [00:00<?, ?it/s]\n",
      "  2%|▊                                         | 157/7600 [00:00<00:04, 1558.71it/s]\n",
      "  4%|█▋                                        | 304/7600 [00:00<00:04, 1527.33it/s]\n",
      "  6%|██▌                                       | 455/7600 [00:00<00:04, 1518.74it/s]\n",
      "  8%|███▎                                      | 608/7600 [00:00<00:04, 1518.80it/s]\n",
      " 10%|████▎                                     | 770/7600 [00:00<00:04, 1544.03it/s]\n",
      " 12%|████▉                                     | 899/7600 [00:00<00:05, 1332.92it/s]\n",
      " 13%|█████▌                                   | 1020/7600 [00:00<00:05, 1242.88it/s]\n",
      " 15%|██████▏                                  | 1137/7600 [00:00<00:05, 1082.50it/s]\n",
      " 16%|██████▋                                  | 1244/7600 [00:01<00:06, 1032.83it/s]\n",
      " 18%|███████▎                                 | 1352/7600 [00:01<00:05, 1044.33it/s]\n",
      " 19%|███████▊                                 | 1456/7600 [00:01<00:05, 1025.38it/s]\n",
      " 21%|████████▌                                 | 1559/7600 [00:01<00:06, 994.91it/s]\n",
      " 22%|█████████                                | 1680/7600 [00:01<00:05, 1048.96it/s]\n",
      " 24%|█████████▋                               | 1798/7600 [00:01<00:05, 1079.96it/s]\n",
      " 25%|██████████▎                              | 1918/7600 [00:01<00:05, 1104.99it/s]\n",
      " 27%|██████████▉                              | 2032/7600 [00:01<00:05, 1112.88it/s]\n",
      " 28%|███████████▌                             | 2151/7600 [00:01<00:04, 1132.58it/s]\n",
      " 30%|████████████▏                            | 2265/7600 [00:01<00:04, 1074.83it/s]\n",
      " 32%|█████████████                            | 2410/7600 [00:02<00:04, 1163.24it/s]\n",
      " 34%|█████████████▊                           | 2570/7600 [00:02<00:03, 1261.41it/s]\n",
      " 36%|██████████████▋                          | 2712/7600 [00:02<00:03, 1302.99it/s]\n",
      " 37%|███████████████▎                         | 2846/7600 [00:02<00:03, 1303.30it/s]\n",
      " 39%|████████████████                         | 2979/7600 [00:02<00:03, 1282.09it/s]\n",
      " 41%|████████████████▊                        | 3117/7600 [00:02<00:03, 1307.25it/s]\n",
      " 43%|█████████████████▋                       | 3272/7600 [00:02<00:03, 1369.04it/s]\n",
      " 45%|██████████████████▍                      | 3426/7600 [00:02<00:02, 1413.35it/s]\n",
      " 47%|███████████████████▎                     | 3572/7600 [00:02<00:02, 1423.85it/s]\n",
      " 49%|████████████████████                     | 3716/7600 [00:02<00:02, 1392.59it/s]\n",
      " 51%|████████████████████▊                    | 3859/7600 [00:03<00:02, 1400.56it/s]\n",
      " 53%|█████████████████████▌                   | 4000/7600 [00:03<00:02, 1289.34it/s]\n",
      " 54%|██████████████████████▎                  | 4137/7600 [00:03<00:02, 1309.77it/s]\n",
      " 56%|███████████████████████                  | 4270/7600 [00:03<00:02, 1275.26it/s]\n",
      " 58%|███████████████████████▊                 | 4404/7600 [00:03<00:02, 1291.29it/s]\n",
      " 60%|████████████████████████▍                | 4535/7600 [00:03<00:02, 1267.80it/s]\n",
      " 61%|█████████████████████████▏               | 4663/7600 [00:03<00:02, 1228.60it/s]\n",
      " 63%|█████████████████████████▉               | 4805/7600 [00:03<00:02, 1277.85it/s]\n",
      " 65%|██████████████████████████▌              | 4934/7600 [00:03<00:02, 1224.22it/s]\n",
      " 67%|███████████████████████████▎             | 5074/7600 [00:04<00:01, 1269.49it/s]\n",
      " 68%|████████████████████████████             | 5203/7600 [00:04<00:01, 1261.77it/s]\n",
      " 70%|████████████████████████████▊            | 5339/7600 [00:04<00:01, 1287.03it/s]\n",
      " 72%|█████████████████████████████▌           | 5469/7600 [00:04<00:01, 1284.29it/s]\n",
      " 74%|██████████████████████████████▏          | 5598/7600 [00:04<00:01, 1253.35it/s]\n",
      " 76%|███████████████████████████████          | 5753/7600 [00:04<00:01, 1327.20it/s]\n",
      " 78%|███████████████████████████████▊         | 5903/7600 [00:04<00:01, 1371.95it/s]\n",
      " 80%|████████████████████████████████▋        | 6065/7600 [00:04<00:01, 1435.06it/s]\n",
      " 82%|█████████████████████████████████▌       | 6231/7600 [00:04<00:00, 1492.93it/s]\n",
      " 84%|██████████████████████████████████▍      | 6383/7600 [00:04<00:00, 1467.58it/s]\n",
      " 86%|███████████████████████████████████▏     | 6532/7600 [00:05<00:00, 1381.17it/s]\n",
      " 88%|████████████████████████████████████     | 6687/7600 [00:05<00:00, 1424.94it/s]\n",
      " 90%|████████████████████████████████████▉    | 6838/7600 [00:05<00:00, 1442.39it/s]\n",
      " 92%|█████████████████████████████████████▋   | 6984/7600 [00:05<00:00, 1415.17it/s]\n",
      " 94%|██████████████████████████████████████▍  | 7128/7600 [00:05<00:00, 1419.45it/s]\n",
      " 96%|███████████████████████████████████████▏ | 7271/7600 [00:05<00:00, 1406.97it/s]\n",
      " 98%|████████████████████████████████████████ | 7418/7600 [00:05<00:00, 1422.28it/s]\n",
      "100%|█████████████████████████████████████████| 7600/7600 [00:05<00:00, 1301.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7577\n",
      "3988\n",
      "3589\n",
      "训练集有400个数据\n",
      "训练集有100个数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:   0%|                 | 0/20 [00:00<?, ?it/s]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:   5%|▍        | 1/20 [00:20<06:31, 20.60s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  10%|▉        | 2/20 [00:39<06:02, 20.12s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  15%|█▎       | 3/20 [00:55<05:18, 18.74s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  20%|█▊       | 4/20 [01:10<04:42, 17.63s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  25%|██▎      | 5/20 [01:27<04:21, 17.45s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  30%|██▋      | 6/20 [01:42<03:55, 16.80s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  35%|███▏     | 7/20 [01:57<03:30, 16.15s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  40%|███▌     | 8/20 [02:12<03:10, 15.84s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  45%|████     | 9/20 [02:28<02:56, 16.07s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  50%|████    | 10/20 [02:43<02:35, 15.54s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  55%|████▍   | 11/20 [02:58<02:18, 15.41s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  60%|████▊   | 12/20 [03:12<02:00, 15.11s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  65%|█████▏  | 13/20 [03:27<01:44, 14.90s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  70%|█████▌  | 14/20 [03:41<01:29, 14.84s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  75%|██████  | 15/20 [03:57<01:15, 15.02s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  80%|██████▍ | 16/20 [04:12<01:00, 15.05s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  85%|██████▊ | 17/20 [04:27<00:45, 15.00s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  90%|███████▏| 18/20 [04:42<00:30, 15.02s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0:  95%|███████▌| 19/20 [04:56<00:14, 14.88s/it]\n",
      "Epoch: 0| Train Loss: 0| Test Loss: 0: 100%|████████| 20/20 [05:11<00:00, 15.56s/it]\n",
      "\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:   0%| | 0/20 [00:00<?, ?it/s]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:   5%| | 1/20 [00:14<04:32, 14.34s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  10%| | 2/20 [00:29<04:21, 14.52s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  15%|▏| 3/20 [00:44<04:10, 14.72s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  20%|▏| 4/20 [00:59<03:57, 14.87s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  25%|▎| 5/20 [01:14<03:43, 14.89s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  30%|▎| 6/20 [01:29<03:27, 14.85s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  35%|▎| 7/20 [01:44<03:16, 15.08s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  40%|▍| 8/20 [02:00<03:00, 15.08s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  45%|▍| 9/20 [02:14<02:42, 14.75s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  50%|▌| 10/20 [02:28<02:25, 14.58s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  55%|▌| 11/20 [02:42<02:11, 14.63s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  60%|▌| 12/20 [02:58<01:58, 14.83s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  65%|▋| 13/20 [03:13<01:44, 14.88s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  70%|▋| 14/20 [03:27<01:28, 14.74s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  75%|▊| 15/20 [03:42<01:14, 14.88s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  80%|▊| 16/20 [03:57<00:59, 14.92s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  85%|▊| 17/20 [04:13<00:45, 15.01s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  90%|▉| 18/20 [04:28<00:29, 14.96s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591:  95%|▉| 19/20 [04:43<00:15, 15.04s/it]\n",
      "Epoch: 1| Train Loss: 0.08669613434467464| Test Loss: 0.03694055244326591: 100%|█| 20/20 [04:58<00:00, 14.91s/it]\n",
      "\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:   0%| | 0/20 [00:00<?, ?it/s]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:   5%| | 1/20 [00:14<04:41, 14.82s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  10%| | 2/20 [00:29<04:26, 14.80s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  15%|▏| 3/20 [00:44<04:11, 14.81s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  20%|▏| 4/20 [00:59<03:58, 14.88s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  25%|▎| 5/20 [01:13<03:40, 14.69s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  30%|▎| 6/20 [01:27<03:23, 14.54s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  35%|▎| 7/20 [01:42<03:07, 14.44s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  40%|▍| 8/20 [01:56<02:53, 14.49s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  45%|▍| 9/20 [02:11<02:39, 14.47s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  50%|▌| 10/20 [02:25<02:24, 14.48s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  55%|▌| 11/20 [02:39<02:09, 14.37s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  60%|▌| 12/20 [02:53<01:54, 14.32s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  65%|▋| 13/20 [03:08<01:40, 14.31s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  70%|▋| 14/20 [03:22<01:26, 14.41s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  75%|▊| 15/20 [03:39<01:15, 15.19s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  80%|▊| 16/20 [03:54<01:00, 15.07s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  85%|▊| 17/20 [04:10<00:46, 15.34s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  90%|▉| 18/20 [04:25<00:30, 15.34s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785:  95%|▉| 19/20 [04:41<00:15, 15.35s/it]\n",
      "Epoch: 2| Train Loss: 0.03294273778796196| Test Loss: 0.03275608122348785: 100%|█| 20/20 [04:56<00:00, 14.83s/it]\n",
      "\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:   0%| | 0/20 [00:00<?, ?it/s]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:   5%| | 1/20 [00:14<04:43, 14.92s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  10%| | 2/20 [00:30<04:30, 15.01s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  15%|▏| 3/20 [00:44<04:12, 14.88s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  20%|▏| 4/20 [01:00<04:01, 15.08s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  25%|▎| 5/20 [01:15<03:45, 15.05s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  30%|▎| 6/20 [01:30<03:31, 15.10s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  35%|▎| 7/20 [01:46<03:18, 15.24s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  40%|▍| 8/20 [02:01<03:02, 15.22s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  45%|▍| 9/20 [02:16<02:48, 15.34s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  50%|▌| 10/20 [02:33<02:36, 15.67s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  55%|▌| 11/20 [02:47<02:18, 15.36s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  60%|▌| 12/20 [03:04<02:05, 15.64s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  65%|▋| 13/20 [03:20<01:50, 15.85s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  70%|▋| 14/20 [03:35<01:34, 15.70s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  75%|▊| 15/20 [03:50<01:17, 15.45s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  80%|▊| 16/20 [04:05<01:00, 15.17s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  85%|▊| 17/20 [04:20<00:45, 15.28s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  90%|▉| 18/20 [04:35<00:30, 15.01s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659:  95%|▉| 19/20 [04:49<00:14, 14.81s/it]\n",
      "Epoch: 3| Train Loss: 0.03433741271495819| Test Loss: 0.0322025316953659: 100%|█| 20/20 [05:03<00:00, 15.18s/it]\n",
      "\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:   0%| | 0/20 [00:00<?, ?it/s]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:   5%| | 1/20 [00:14<04:36, 14.56s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  10%| | 2/20 [00:28<04:20, 14.46s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  15%|▏| 3/20 [00:43<04:04, 14.40s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  20%|▏| 4/20 [00:57<03:49, 14.36s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  25%|▎| 5/20 [01:11<03:34, 14.33s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  30%|▎| 6/20 [01:25<03:20, 14.35s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  35%|▎| 7/20 [01:40<03:06, 14.38s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  40%|▍| 8/20 [01:54<02:52, 14.36s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  45%|▍| 9/20 [02:09<02:38, 14.36s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  50%|▌| 10/20 [02:24<02:25, 14.58s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  55%|▌| 11/20 [02:38<02:11, 14.63s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  60%|▌| 12/20 [02:53<01:55, 14.47s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  65%|▋| 13/20 [03:08<01:42, 14.63s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  70%|▋| 14/20 [03:24<01:30, 15.07s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  75%|▊| 15/20 [03:40<01:16, 15.34s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  80%|▊| 16/20 [03:54<01:00, 15.10s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  85%|▊| 17/20 [04:08<00:44, 14.85s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  90%|▉| 18/20 [04:23<00:29, 14.63s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446:  95%|▉| 19/20 [04:37<00:14, 14.57s/it]\n",
      "Epoch: 4| Train Loss: 0.033328261077404026| Test Loss: 0.030316202640533446: 100%|█| 20/20 [04:52<00:00, 14.63s/it]\n",
      "\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:   0%| | 0/20 [00:00<?, ?it/s]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:   5%| | 1/20 [00:14<04:37, 14.63s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  10%| | 2/20 [00:31<04:33, 15.18s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  15%|▏| 3/20 [00:46<04:20, 15.33s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  20%|▏| 4/20 [01:02<04:04, 15.30s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  25%|▎| 5/20 [01:17<03:52, 15.48s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  30%|▎| 6/20 [01:33<03:36, 15.46s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  35%|▎| 7/20 [01:48<03:18, 15.28s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  40%|▍| 8/20 [02:02<03:00, 15.05s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  45%|▍| 9/20 [02:17<02:45, 15.02s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  50%|▌| 10/20 [02:34<02:35, 15.55s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  55%|▌| 11/20 [02:49<02:18, 15.39s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  60%|▌| 12/20 [03:04<02:03, 15.41s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  65%|▋| 13/20 [03:20<01:48, 15.46s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  70%|▋| 14/20 [03:35<01:31, 15.31s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  75%|▊| 15/20 [03:50<01:16, 15.27s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  80%|▊| 16/20 [04:05<01:00, 15.12s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  85%|▊| 17/20 [04:20<00:45, 15.20s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  90%|▉| 18/20 [04:36<00:30, 15.33s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398:  95%|▉| 19/20 [04:51<00:15, 15.35s/it]\n",
      "Epoch: 5| Train Loss: 0.03257985949516296| Test Loss: 0.027798223495483398: 100%|█| 20/20 [05:07<00:00, 15.38s/it]\n",
      "\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:   0%| | 0/20 [00:00<?, ?it/s]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:   5%| | 1/20 [00:15<04:52, 15.39s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  10%| | 2/20 [00:30<04:37, 15.41s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  15%|▏| 3/20 [00:46<04:23, 15.49s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  20%|▏| 4/20 [01:02<04:11, 15.69s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  25%|▎| 5/20 [01:18<03:55, 15.73s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  30%|▎| 6/20 [01:33<03:36, 15.50s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  35%|▎| 7/20 [01:48<03:20, 15.45s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  40%|▍| 8/20 [02:04<03:04, 15.41s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  45%|▍| 9/20 [02:19<02:49, 15.44s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  50%|▌| 10/20 [02:35<02:35, 15.58s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  55%|▌| 11/20 [02:51<02:22, 15.80s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  60%|▌| 12/20 [03:06<02:04, 15.58s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  65%|▋| 13/20 [03:21<01:47, 15.31s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  70%|▋| 14/20 [03:36<01:30, 15.10s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  75%|▊| 15/20 [03:51<01:15, 15.06s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  80%|▊| 16/20 [04:06<01:00, 15.00s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  85%|▊| 17/20 [04:20<00:44, 14.93s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  90%|▉| 18/20 [04:35<00:29, 14.81s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922:  95%|▉| 19/20 [04:49<00:14, 14.69s/it]\n",
      "Epoch: 6| Train Loss: 0.03055223487317562| Test Loss: 0.024352012872695922: 100%|█| 20/20 [05:05<00:00, 15.27s/it]\n",
      "\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:   0%| | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:   5%| | 1/20 [00:14<04:40, 14.76s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  10%| | 2/20 [00:29<04:23, 14.65s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  15%|▏| 3/20 [00:44<04:10, 14.74s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  20%|▏| 4/20 [00:58<03:55, 14.71s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  25%|▎| 5/20 [01:13<03:39, 14.66s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  30%|▎| 6/20 [01:27<03:24, 14.63s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  35%|▎| 7/20 [01:42<03:10, 14.66s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  40%|▍| 8/20 [01:57<02:57, 14.77s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  45%|▍| 9/20 [02:13<02:45, 15.07s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  50%|▌| 10/20 [02:28<02:31, 15.14s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  55%|▌| 11/20 [02:43<02:14, 14.98s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  60%|▌| 12/20 [02:58<01:59, 14.92s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  65%|▋| 13/20 [03:12<01:43, 14.79s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  70%|▋| 14/20 [03:26<01:28, 14.68s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  75%|▊| 15/20 [03:41<01:13, 14.63s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  80%|▊| 16/20 [03:56<00:58, 14.64s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  85%|▊| 17/20 [04:11<00:44, 14.73s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  90%|▉| 18/20 [04:26<00:29, 14.83s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032:  95%|▉| 19/20 [04:41<00:15, 15.12s/it]\n",
      "Epoch: 7| Train Loss: 0.030436553582549094| Test Loss: 0.021654999256134032: 100%|█| 20/20 [04:58<00:00, 14.92s/it]\n",
      "\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:   0%| | 0/20 [00:00<?, ?it/s]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:   5%| | 1/20 [00:16<05:07, 16.19s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  10%| | 2/20 [00:31<04:46, 15.94s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  15%|▏| 3/20 [00:46<04:26, 15.68s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  20%|▏| 4/20 [01:02<04:13, 15.83s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  25%|▎| 5/20 [01:18<03:54, 15.64s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  30%|▎| 6/20 [01:33<03:38, 15.60s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  35%|▎| 7/20 [01:49<03:24, 15.69s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  40%|▍| 8/20 [02:06<03:14, 16.19s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  45%|▍| 9/20 [02:24<03:03, 16.72s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  50%|▌| 10/20 [02:40<02:44, 16.42s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  55%|▌| 11/20 [02:56<02:26, 16.23s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  60%|▌| 12/20 [03:11<02:06, 15.87s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  65%|▋| 13/20 [03:27<01:52, 16.02s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  70%|▋| 14/20 [03:43<01:34, 15.82s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  75%|▊| 15/20 [03:57<01:17, 15.46s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  80%|▊| 16/20 [04:12<01:01, 15.39s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  85%|▊| 17/20 [04:28<00:46, 15.53s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  90%|▉| 18/20 [04:45<00:31, 15.78s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184:  95%|▉| 19/20 [05:00<00:15, 15.56s/it]\n",
      "Epoch: 8| Train Loss: 0.028710911273956297| Test Loss: 0.019306253492832184: 100%|█| 20/20 [05:15<00:00, 15.76s/it]\n",
      "\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:   0%| | 0/20 [00:00<?, ?it/s]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:   5%| | 1/20 [00:14<04:40, 14.77s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  10%| | 2/20 [00:30<04:29, 14.95s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  15%|▏| 3/20 [00:46<04:19, 15.27s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  20%|▏| 4/20 [01:02<04:07, 15.49s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  25%|▎| 5/20 [01:17<03:52, 15.51s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  30%|▎| 6/20 [01:33<03:40, 15.72s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  35%|▎| 7/20 [01:49<03:24, 15.76s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  40%|▍| 8/20 [02:06<03:13, 16.15s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  45%|▍| 9/20 [02:22<02:56, 16.05s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  50%|▌| 10/20 [02:39<02:43, 16.31s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  55%|▌| 11/20 [02:55<02:26, 16.33s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  60%|▌| 12/20 [03:11<02:09, 16.17s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  65%|▋| 13/20 [03:26<01:50, 15.85s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  70%|▋| 14/20 [03:42<01:34, 15.71s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  75%|▊| 15/20 [03:57<01:17, 15.47s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  80%|▊| 16/20 [04:12<01:01, 15.47s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  85%|▊| 17/20 [04:29<00:47, 15.78s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  90%|▉| 18/20 [04:45<00:31, 15.90s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668:  95%|▉| 19/20 [05:00<00:15, 15.74s/it]\n",
      "Epoch: 9| Train Loss: 0.025719750225543975| Test Loss: 0.018114234209060668: 100%|█| 20/20 [05:15<00:00, 15.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "seed_torch(22)\n",
    "epoch = 10\n",
    "batch_size = 20\n",
    "early_stop = 5\n",
    "test_loss_list = []\n",
    "train_loss_list = []\n",
    "\n",
    "# 初始化模型\n",
    "model = LSTM()\n",
    "model_ = LSTM()\n",
    "\n",
    "# 数据处理部分\n",
    "gd = GetData()\n",
    "x, y = gd.split_sen()\n",
    "pos = y.count(1)\n",
    "neg = y.count(0)\n",
    "pos_train = int(pos*0.9)\n",
    "neg_train = int(neg*0.9)\n",
    "x1 = x[:pos]  # 3988   --- 3589train 399test\n",
    "y1 = y[:pos]\n",
    "x0 = x[pos:]  # 3589   ---- 3230train 359test\n",
    "y0 = y[pos:]\n",
    "\n",
    "train_x = x0[:neg_train] + x1[:pos_train]\n",
    "train_y = y0[:neg_train] + y1[:pos_train]\n",
    "print(\"训练集有\"+str(len(train_x))+\"个数据\")\n",
    "\n",
    "# c = list(zip(train_x, train_y))\n",
    "# random.shuffle(c)\n",
    "# c = random.sample(c, 50)\n",
    "# train_x[:], train_y[:] = zip(*c)\n",
    "\n",
    "test_x = x0[neg_train:] + x1[pos_train:]\n",
    "test_y = y0[neg_train:] + y1[pos_train:]\n",
    "print(\"测试集有\"+str(len(test_x))+\"个数据\")\n",
    "\n",
    "bert = GetBERT()\n",
    "x_train = bert(train_x)\n",
    "x_test = bert(test_x)\n",
    "y_train = torch.tensor(train_y).float()\n",
    "y_test = torch.tensor(test_y).float()\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_dataLoader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "test_dataLoader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# 损失函数和优化器\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(epoch, train_dataLoader, test_dataLoader)\n",
    "p, y, test_loss, test_loss_list = test_model(test_dataLoader)\n",
    "ans = []\n",
    "for t in p:\n",
    "    if t[0]>t[1]:\n",
    "        ans.append(0)\n",
    "    else:\n",
    "        ans.append(1)\n",
    "print(accuracy_score(ans,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T11:40:00.429468Z",
     "start_time": "2021-06-21T11:39:58.612358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0785],\n",
       "        [-0.0127]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GetBERT()\n",
    "x = model([['味道好饭量足'],['喜欢烤羊肉']])\n",
    "lstm = LSTM()\n",
    "lstm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T08:26:13.498073Z",
     "start_time": "2021-06-22T08:26:11.671156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "消极\n"
     ]
    }
   ],
   "source": [
    "bert = GetBERT()\n",
    "model = LSTM()\n",
    "model.load_state_dict(torch.load(\"lstm_.pth\"))\n",
    "model.eval()\n",
    "for t in model(bert(['实在是太难吃了'])):\n",
    "    if t[0]>t[1]:\n",
    "        print(\"消极\")\n",
    "    else:\n",
    "        print(\"积极\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T11:38:56.220737Z",
     "start_time": "2021-06-21T11:38:51.156197Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T11:40:54.698698Z",
     "start_time": "2021-06-21T11:40:54.686731Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
