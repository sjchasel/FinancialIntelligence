{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:13:44.300433Z",
     "start_time": "2021-06-15T12:13:44.280527Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T04:20:16.202157Z",
     "start_time": "2021-06-15T04:20:16.189234Z"
    }
   },
   "outputs": [],
   "source": [
    "BERT_PATH = 'bert-base-chinese/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:27:38.150981Z",
     "start_time": "2021-06-15T12:27:38.127965Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"C:/Users/12968/Desktop/chinese-bert-wwm-ext\")\n",
    "        self.bert = BertModel.from_pretrained(\"C:/Users/12968/Desktop/chinese-bert-wwm-ext\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.fc = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, sentence_lists):\n",
    "        \"\"\"\n",
    "        输入句子列表(去掉了停用词的)\n",
    "        \"\"\"\n",
    "        sentence_lists = [' '.join(x) for x in sentence_lists]\n",
    "        print(sentence_lists)\n",
    "        ids = self.bert_tokenizer(sentence_lists, padding=True, return_tensors=\"pt\")\n",
    "        print(ids)\n",
    "        inputs = ids['input_ids']\n",
    "        print(inputs)\n",
    "\n",
    "        embeddings = self.bert(inputs)\n",
    "        return embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:59:59.432204Z",
     "start_time": "2021-06-15T12:59:59.397338Z"
    }
   },
   "outputs": [],
   "source": [
    "class Pre:\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        输入一个文本\n",
    "        \"\"\"\n",
    "        self.puncs_coarse = ['。', '!', '；', '？', '……', '\\n',' ']\n",
    "        self.text = text\n",
    "        self.stopwords = self.deal_wrap('dict/stop1205.txt')\n",
    "    \n",
    "    def segment(self, sentence):\n",
    "        sentence_seged = jieba.cut(sentence.strip())\n",
    "        outstr = ''\n",
    "        for word in sentence_seged:\n",
    "            if word not in stopwords:\n",
    "                if word != '\\t':\n",
    "                    outstr += word\n",
    "                    outstr += \" \"\n",
    "        word_list = outstr.split(' ')\n",
    "        pattern = '[A-Za-z]*[0-9]*[\\'\\\"\\%.\\s\\@\\!\\#\\$\\^\\&\\*\\(\\)\\-\\<\\>\\?\\/\\,\\~\\`\\:\\;]*[：；”“ ‘’+-——！，。？、~@#￥%……&*（）【】]*'\n",
    "        t = [re.sub(pattern, \"\", x.strip()) for x in word_list]\n",
    "        t = [x for x in t if x != '']\n",
    "        return ''.join(t)\n",
    "    \n",
    "    def deal_wrap(self, filedict):\n",
    "        temp = []\n",
    "        for x in open(filedict, 'r', encoding='utf-8').readlines():\n",
    "            temp.append(x.strip())\n",
    "        return temp\n",
    "        \n",
    "    def split_sentence_coarse(self):\n",
    "        \"\"\"\n",
    "        按照。！？“”等中文完整句子语义来分句\n",
    "        1. 去除换行符、多余的空格、百分号\n",
    "        2. 分句，存入列表\n",
    "        :return:装着每个句子的列表（包括标点符号）\n",
    "        \"\"\"\n",
    "        text = self.text\n",
    "        sentences = []\n",
    "        start = 0\n",
    "        for i in range(len(text)):\n",
    "            if text[i] in self.puncs_coarse:\n",
    "                sentences.append(text[start:i + 1])\n",
    "                start = i + 1\n",
    "        return sentences\n",
    "    \n",
    "    def get_keywords(self, data):\n",
    "        \"\"\"\n",
    "        如果句子太长，就进行关键词提取\n",
    "        \"\"\"\n",
    "        from jieba import analyse\n",
    "        textrank = analyse.textrank\n",
    "        keywords = textrank(data, topK=8)\n",
    "        return ''.join(keywords)\n",
    "\n",
    "    def preprocess(self):\n",
    "        # 分句\n",
    "        sentences = self.split_sentence_coarse()\n",
    "        # 对每个句子，去除里面的停用词，再连起来\n",
    "        # 对每个句子，如果句子太长，长度大于20（我随便定的），就抽取八个关键词连起来\n",
    "        new_sent = []\n",
    "        for i in sentences:\n",
    "            i = self.segment(i)\n",
    "            if len(i) > 20:\n",
    "                i = self.get_keywords(i)\n",
    "            if i != '':\n",
    "                new_sent.append(i)\n",
    "        return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:51:03.193315Z",
     "start_time": "2021-06-15T12:51:03.174404Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T04:50:51.178543Z",
     "start_time": "2021-06-15T04:50:51.156641Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
