{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T11:55:19.839253Z",
     "start_time": "2021-06-27T11:55:15.860392Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy as copy\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "def seed_torch(seed=1122):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "BERT_PATH = 'bert-base-chinese/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T11:55:25.555468Z",
     "start_time": "2021-06-27T11:55:21.979183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "一只酸奶牛（财大店）.xlsx\n",
      "三牦记.牛肉火锅.csv\n",
      "九孔莲藕汤锅（总店）.csv\n",
      "乡村基（国色天香店）.csv\n",
      "书亦烧仙草(温江财大二店).xlsx\n",
      "五味鲜肉抄手（温江总店）.csv\n",
      "伍氏猪脚（柳浪湾店）.csv\n",
      "众里寻她·千百度自助烤肉（温江店）.csv\n",
      "佘记全家福芋儿鸡（温江店）.xlsx\n",
      "北木南和烤肉（西南财大店）.xlsx\n",
      "华莱士评论采集.csv\n",
      "台新石头火锅（温江店）.csv\n",
      "司乎 日式鲜吐司（新光天地店）.csv\n",
      "吉布鲁牛排海鲜自助（温江合生汇店）.xlsx\n",
      "嘎嘎鸭脑壳（温江南浦店）.xlsx\n",
      "在首尔韩国料理•自助烤肉（温江店）.csv\n",
      "大美蓉城自助火锅烤肉（温江店）.csv\n",
      "奇味干锅馆闷锅乡.xlsx\n",
      "好利来（温江合生汇店）.csv\n",
      "守柴炉烤鸭（温江新尚天地店）.csv\n",
      "官人桥蒙自过桥米线.csv\n",
      "宾GO101牛排披萨意面.xlsx\n",
      "尼克熊西餐（财大东门店）.csv\n",
      "屋头串串（临江路店）.csv\n",
      "川西坝子火锅（温江2.0店）.csv\n",
      "巴色鱼捞（温江直营店）.xlsx\n",
      "柳飘飘亚洲烤肉集合店.xlsx\n",
      "正新鸡排（繁华广场店）.xlsx\n",
      "汉堡王（温江新尚天地店）.csv\n",
      "江南道.xlsx\n",
      "江南道自助烤肉海鲜涮锅（温江大学城店）.csv\n",
      "泥巴小院市井火锅（温江店）.csv\n",
      "泰澜得.csv\n",
      "泰澜得.csv.csv\n",
      "渝味晓宇火锅（温江店）.csv\n",
      "烤匠评论采集_790.csv\n",
      "烤鱼匠麻辣烤鱼.xlsx\n",
      "爱达乐蛋糕.xlsx\n",
      "牛蹄筋老面馆（财大店）.xlsx\n",
      "独二家私房面.xlsx\n",
      "猪圈小米辣（温江1店）.csv\n",
      "王婆婆大牌檔.csv\n",
      "码头故事火锅.csv\n",
      "紫燕百味鸡.xlsx\n",
      "芭夯兔.xlsx\n",
      "芭莉与彩虹西餐厅（成都合生汇店）.csv\n",
      "蛙三泡椒牛蛙（温江合生汇店）.xlsx\n",
      "蜀大侠去重评论采集.csv\n",
      "蜜语之恋.xlsx\n",
      "裕昌火锅（德通桥路总店）.csv\n",
      "诺曼汀艺术烘焙（财大店）.csv\n",
      "谦和清心素食（温江店）.csv\n",
      "豪客来牛排（成都温江新尚天地店）.csv\n",
      "辛麻道火锅.xlsx\n",
      "锦城大院火锅.csv\n",
      "锦荷苑自助火锅.csv\n",
      "院坝记忆牦牛串串·龙虾·烧烤（鱼凫路总店）.csv\n",
      "雅府正红木桶鱼.csv\n",
      "青椒鱼（温江大学城店）.xlsx\n",
      "韩呈一品韩式自助（温江店）.xlsx\n",
      "韩国小胖料理自助烤肉.csv\n",
      "韩婆婆小院烧烤.xlsx\n",
      "香街坊汤锅馆（国色天香店）.csv\n",
      "鱼契.烤鱼（西南财大总店）.csv\n",
      "鸡毛店.csv\n",
      "麦香园.xlsx\n",
      "黄记煌三汁焖锅（温江合生汇店）.csv\n",
      "龙腾庭院火锅（恒大新城店）.csv\n",
      "俭让佬妈串串.xlsx\n"
     ]
    }
   ],
   "source": [
    "datalist = os.listdir('comment_data')\n",
    "print(len(datalist))\n",
    "datalst = []\n",
    "for i in datalist:\n",
    "    print(i)\n",
    "    try:\n",
    "        if i[-1] == 'x':\n",
    "            data = pd.read_excel('comment_data/'+i).drop_duplicates(subset=['店铺名称','用户名','评论'],keep='first')\n",
    "        else:\n",
    "            data = pd.read_csv('comment_data/'+i).drop_duplicates(subset=['店铺名称','用户名','评论'],keep='first')\n",
    "        datalst.append(data)\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T12:00:09.649748Z",
     "start_time": "2021-06-27T12:00:04.792442Z"
    }
   },
   "outputs": [],
   "source": [
    "for data in datalst:\n",
    "    data.dropna(axis=0,subset = [\"评论\"]) \n",
    "pd.concat(datalst).to_csv(\"全部评论数据.csv\",encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试情感分析模型是否有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T12:42:13.694256Z",
     "start_time": "2021-06-22T12:42:12.988194Z"
    }
   },
   "outputs": [],
   "source": [
    "comment = pd.read_csv('全部评论数据.csv')\n",
    "comment = comment[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T12:38:50.633551Z",
     "start_time": "2021-06-22T12:38:50.413098Z"
    }
   },
   "outputs": [],
   "source": [
    "class GetBERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GetBERT, self).__init__()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"C:/Users/12968/Desktop/chinese-bert-wwm-ext\")\n",
    "        self.bert = BertModel.from_pretrained(\"C:/Users/12968/Desktop/chinese-bert-wwm-ext\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    def forward(self, sentence_lists):\n",
    "        \"\"\"\n",
    "        输入句子列表(去掉了停用词的)\n",
    "        \"\"\"\n",
    "        sentence_lists = [' '.join(x) for x in sentence_lists]\n",
    "        #print('sentence_lists:'+str(sentence_lists))\n",
    "        ids = self.bert_tokenizer(sentence_lists, padding=True, return_tensors=\"pt\")\n",
    "        #print('ids:'+str(ids))\n",
    "        inputs = ids['input_ids']\n",
    "        #print('inputs:'+str(inputs))\n",
    "\n",
    "        embeddings = self.bert(inputs)\n",
    "        #print(str(embeddings[0].shape))\n",
    "        x = embeddings[0] #1 * 768\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class Pre:\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        输入一个文本\n",
    "        \"\"\"\n",
    "        self.puncs_coarse = ['。', '!', '；', '？', '……', '\\n',' ']\n",
    "        self.text = text\n",
    "        self.stopwords = self.deal_wrap('dict/stop1205.txt')\n",
    "    \n",
    "    def segment(self, sentence):\n",
    "        sentence_seged = jieba.cut(sentence.strip())\n",
    "        outstr = ''\n",
    "        for word in sentence_seged:\n",
    "            if word not in self.stopwords:\n",
    "                if word != '\\t':\n",
    "                    outstr += word\n",
    "                    outstr += \" \"\n",
    "        word_list = outstr.split(' ')\n",
    "        pattern = '[A-Za-z]*[0-9]*[\\'\\\"\\%.\\s\\@\\!\\#\\$\\^\\&\\*\\(\\)\\-\\<\\>\\?\\/\\,\\~\\`\\:\\;]*[：；”“ ‘’+-——！，。？、~@#￥%……&*（）【】]*'\n",
    "        t = [re.sub(pattern, \"\", x.strip()) for x in word_list]\n",
    "        t = [x for x in t if x != '']\n",
    "        return ''.join(t)\n",
    "    \n",
    "    def deal_wrap(self, filedict):\n",
    "        temp = []\n",
    "        for x in open(filedict, 'r', encoding='utf-8').readlines():\n",
    "            temp.append(x.strip())\n",
    "        return temp\n",
    "        \n",
    "    def split_sentence_coarse(self):\n",
    "        \"\"\"\n",
    "        按照。！？“”等中文完整句子语义来分句\n",
    "        1. 去除换行符、多余的空格、百分号\n",
    "        2. 分句，存入列表\n",
    "        :return:装着每个句子的列表（包括标点符号）\n",
    "        \"\"\"\n",
    "        \n",
    "        text = self.text\n",
    "        sentences = []\n",
    "        start = 0\n",
    "        for i in range(len(text)):\n",
    "            if text[i] in self.puncs_coarse:\n",
    "                sentences.append(text[start:i + 1])\n",
    "                start = i + 1\n",
    "        if start == 0:\n",
    "            sentences.append(text)\n",
    "        return sentences\n",
    "    \n",
    "    def get_keywords(self, data):\n",
    "        \"\"\"\n",
    "        如果句子太长，就进行关键词提取\n",
    "        \"\"\"\n",
    "        from jieba import analyse\n",
    "        textrank = analyse.textrank\n",
    "        keywords = textrank(data, topK=8)\n",
    "        return ''.join(keywords)\n",
    "\n",
    "    def preprocess(self):\n",
    "        # 分句\n",
    "        sentences = self.split_sentence_coarse()\n",
    "        # 对每个句子，去除里面的停用词，再连起来\n",
    "        # 对每个句子，如果句子太长，长度大于20（我随便定的），就抽取八个关键词连起来\n",
    "        new_sent = []\n",
    "        for i in sentences:\n",
    "            if len(i) < 5:\n",
    "                new_sent.append(i)\n",
    "                continue\n",
    "            i = self.segment(i)\n",
    "            if len(i) > 25:\n",
    "                i = self.get_keywords(i)\n",
    "            if i != '':\n",
    "                new_sent.append(i)\n",
    "        return new_sent\n",
    "    \n",
    "class GetData():\n",
    "    def __init__(self,pos=4000, neg=3600):\n",
    "        data = pd.read_excel('E:/FinancialIntelligence/sentiment_classify_data/comments_raw_v1.xls')\n",
    "        data = data[data['score']!=3].reset_index()\n",
    "        data['label'] = data['score'].map(lambda a : 1 if a in [4,5] else 0) \n",
    "        data.drop(['id','score'],inplace=True,axis=1)\n",
    "        data['content'] = [str(i) for i in list(data['content'])]\n",
    "        # 原数据标签为0（负向情感）的数据有3632条，正向情感的有57262条\n",
    "        data1 = data[data['label']==1].sample(pos)\n",
    "        data0 = data[data['label']==0].sample(neg)\n",
    "        data = pd.concat([data1,data0],axis=0,ignore_index=True)\n",
    "        self.data = data\n",
    "    \n",
    "    def split_sen(self):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in trange(len(self.data)):\n",
    "            p = Pre(self.data['content'][i])\n",
    "            sen_lst = p.preprocess()\n",
    "            if sen_lst == []:\n",
    "                continue\n",
    "            x.append(sen_lst)\n",
    "            y.append(self.data['label'][i])\n",
    "        print(len(x))\n",
    "        print(y.count(1))\n",
    "        print(y.count(0))\n",
    "        return x, y\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm_layer = nn.LSTM(input_size=768, hidden_size=128, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(in_features=128, out_features=2, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1, (h_n, h_c) = self.lstm_layer(x)\n",
    "        a, b, c = h_n.shape\n",
    "        out = self.linear_layer(h_n.reshape(a*b, c))\n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        return out\n",
    "    \n",
    "def train_model(epoch, train_dataLoader, test_dataLoader):\n",
    "    # 训练模型\n",
    "    best_model = None\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    best_loss = 100\n",
    "    epoch_cnt = 0\n",
    "    for _ in range(epoch):\n",
    "        total_train_loss = 0\n",
    "        total_train_num = 0\n",
    "        total_test_loss = 0\n",
    "        total_test_num = 0\n",
    "        for x, y in tqdm(train_dataLoader,\n",
    "                         desc='Epoch: {}| Train Loss: {}| Test Loss: {}'.format(_, train_loss, test_loss)):\n",
    "        #for x, y in train_dataLoader:\n",
    "            x_num = len(x)\n",
    "            p = model(x)\n",
    "            loss = loss_func(p, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_num += x_num\n",
    "        train_loss = total_train_loss / total_train_num\n",
    "        train_loss_list.append(train_loss)\n",
    "        for x, y in test_dataLoader:\n",
    "            x_num = len(x)\n",
    "            p = model(x)\n",
    "            loss = loss_func(p, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_num += x_num\n",
    "        test_loss = total_test_loss / total_test_num\n",
    "        test_loss_list.append(test_loss)\n",
    "        \n",
    "        # early stop\n",
    "        if best_loss > test_loss:\n",
    "            best_loss = test_loss\n",
    "            best_model = copy(model)\n",
    "            torch.save(best_model.state_dict(), 'lstm_.pth')\n",
    "            epoch_cnt = 0\n",
    "        else:\n",
    "            epoch_cnt += 1\n",
    "            \n",
    "        if epoch_cnt > early_stop:\n",
    "            torch.save(best_model.state_dict(), 'lstm_.pth')\n",
    "            print(\"保存模型\")\n",
    "            #print(best_model.state_dict())\n",
    "            break\n",
    "def test_model(test_dataLoader_):\n",
    "    pred = []\n",
    "    label = []\n",
    "    model_.load_state_dict(torch.load(\"lstm_.pth\"))\n",
    "    model_.eval()\n",
    "    total_test_loss = 0\n",
    "    total_test_num = 0\n",
    "    for x, y in test_dataLoader_:\n",
    "        x_num = len(x)\n",
    "        p = model_(x)\n",
    "#         print('##', len(p), len(y))\n",
    "        loss = loss_func(p, y.long())\n",
    "        total_test_loss += loss.item()\n",
    "        total_test_num += x_num\n",
    "        pred.extend(p.data.squeeze(1).tolist())\n",
    "        label.extend(y.tolist())\n",
    "    test_loss = total_test_loss / total_test_num\n",
    "    # print('##', len(pred), len(label))\n",
    "    return pred, label, test_loss, test_loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T12:43:17.642993Z",
     "start_time": "2021-06-22T12:42:25.817550Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [00:44<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "bert = GetBERT()\n",
    "model = LSTM()\n",
    "model.load_state_dict(torch.load(\"lstm_.pth\"))\n",
    "model.eval()\n",
    "comment['sent'] = 99\n",
    "for i in trange(len(comment)):\n",
    "    res = model(bert([comment.iloc[i,3]]))\n",
    "    for t in res:\n",
    "        if t[0]>t[1]:\n",
    "            comment.iloc[i,4] = 0\n",
    "        else:\n",
    "            comment.iloc[i,4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T12:43:17.768661Z",
     "start_time": "2021-06-22T12:43:17.655959Z"
    }
   },
   "outputs": [],
   "source": [
    "comment.to_excel(\"100条数据测试情感.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T12:42:01.385475Z",
     "start_time": "2021-06-22T12:42:00.676338Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
